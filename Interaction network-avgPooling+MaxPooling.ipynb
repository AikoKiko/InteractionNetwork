{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import setGPU\n",
    "import numpy as np\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "import torch \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data.distributed\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from operator import itemgetter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading completed\n"
     ]
    }
   ],
   "source": [
    "N=188\n",
    "infile = h5py.File('/bigdata/shared/HLS4ML/jetImage.h5', 'r')\n",
    "sorted_pt_constituents = np.array(infile.get('jetConstituentList'))\n",
    "scaled_jets = infile.get('jets') # in any case\n",
    "mass_targets = scaled_jets[:,-6:-1]\n",
    "print(\"Loading completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphNet(\n",
      "  (fr1): Linear(in_features=6, out_features=10, bias=True)\n",
      "  (fr2): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (fr3): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (fo1): Linear(in_features=8, out_features=10, bias=True)\n",
      "  (fo2): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (fo3): Linear(in_features=5, out_features=6, bias=True)\n",
      "  (fc1): Linear(in_features=400, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (fc3): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (avg): AdaptiveAvgPool1d(output_size=200)\n",
      "  (mp): AdaptiveMaxPool1d(output_size=200)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "class GraphNet(nn.Module):\n",
    "    def __init__(self, n_constituents, n_targets, params, hidden):\n",
    "        super(GraphNet, self).__init__()\n",
    "        self.hidden = hidden\n",
    "        self.P = params\n",
    "        self.N = n_constituents\n",
    "        self.Nr = self.N * (self.N - 1)\n",
    "        self.Dr = 0\n",
    "        self.De = 5\n",
    "        self.Dx = 0\n",
    "        self.Do = 6\n",
    "        self.n_targets = n_targets\n",
    "        self.assign_matrices()\n",
    "        self.Ra = Variable(torch.ones(self.Dr, self.Nr))\n",
    "        self.fr1 = nn.Linear(2 * self.P + self.Dr, hidden)\n",
    "        self.fr2 = nn.Linear(hidden, int(hidden/2))\n",
    "        self.fr3 = nn.Linear(int(hidden/2), self.De)\n",
    "        self.fo1 = nn.Linear(self.P + self.Dx + self.De, hidden)\n",
    "        self.fo2 = nn.Linear(hidden, int(hidden/2))\n",
    "        self.fo3 = nn.Linear(int(hidden/2), self.Do)\n",
    "        self.fc1 = nn.Linear(400, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, int(hidden/2))\n",
    "        self.fc3 = nn.Linear(int(hidden/2), self.n_targets)\n",
    "        self.avg = nn.AdaptiveAvgPool1d(200)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(200)\n",
    "    def assign_matrices(self):\n",
    "        self.Rr = torch.zeros(self.N, self.Nr)\n",
    "        self.Rs = torch.zeros(self.N, self.Nr)\n",
    "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.N)) if i[0]!=i[1]]\n",
    "        for i, (r, s) in enumerate(receiver_sender_list):\n",
    "            self.Rr[r, i] = 1\n",
    "            self.Rs[s, i] = 1\n",
    "        self.Rr = Variable(self.Rr).cuda()\n",
    "        self.Rs = Variable(self.Rs).cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=torch.transpose(x, 1, 2).contiguous()\n",
    "        Orr = self.tmul(x, self.Rr)\n",
    "        Ors = self.tmul(x, self.Rs)\n",
    "#         print(\"This is the Orr {}\". format(Orr.shape))\n",
    "#         print(\"This is the Ors {}\". format(Ors.shape))\n",
    "        B = torch.cat([Orr, Ors], 1)\n",
    "#         print(\"This is what after concat, actual B is {}\". format(B.shape))\n",
    "        ### First MLP ###\n",
    "        B = torch.transpose(B, 1, 2).contiguous()\n",
    "#         print(\"size of B after transposed is {}\". format(B.shape))\n",
    "        B = nn.functional.relu(self.fr1(B.view(-1, 2 * self.P + self.Dr)))\n",
    "        B = nn.functional.relu(self.fr2(B))\n",
    "        E = nn.functional.relu(self.fr3(B).view(-1, self.Nr, self.De))\n",
    "#         print(\"size of E {}\". format(E.shape))\n",
    "        del B\n",
    "        E = torch.transpose(E, 1, 2).contiguous()\n",
    "#         print(\"size of E after transposed{}\". format(E.shape))\n",
    "        Ebar = self.tmul(E, torch.transpose(self.Rr, 0, 1).contiguous())\n",
    "        del E\n",
    "        C = torch.cat([x, Ebar], 1)\n",
    "        C = torch.transpose(C, 1, 2).contiguous()\n",
    "        ### Second MLP ###\n",
    "        C = nn.functional.relu(self.fo1(C.view(-1, self.P + self.Dx + self.De)))\n",
    "        C = nn.functional.relu(self.fo2(C))\n",
    "        O = nn.functional.relu(self.fo3(C).view(-1, self.N, self.Do))\n",
    "        del C\n",
    "        ### Classification MLP ###\n",
    "        batch_size,max_len, features = O.size()\n",
    "        O = O.contiguous().view(batch_size, -1).unsqueeze(0)\n",
    "        N = self.avg(O).squeeze(0)\n",
    "        N2 = self.mp(O).squeeze(0)\n",
    "        N = torch.cat((N,N2),1)\n",
    "        N = nn.functional.relu(self.fc1(N))\n",
    "        del O\n",
    "        N = nn.functional.relu(self.fc2(N))\n",
    "        N = self.fc3(N)\n",
    "        return N\n",
    "    def tmul(self, x, y):  #Takes (I * J * K)(K * L) -> I * J * L \n",
    "        x_shape = x.size()\n",
    "        y_shape = y.size()\n",
    "#         print(x_shape)\n",
    "#         print(y_shape)\n",
    "        return torch.mm(x.view(-1, x_shape[2]), y).view(-1, x_shape[1], y_shape[1])\n",
    "\n",
    "    \n",
    "model = GraphNet(n_constituents=188,n_targets=5,params=3, hidden=10)\n",
    "model.cuda()\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////for later usage///////\n",
    "# def get_sample(training, target, choice):\n",
    "#     target_vals = np.argmax(target, axis = 1)\n",
    "#     ind, = np.where(target_vals == choice)\n",
    "#     chosen_ind = np.random.choice(ind, 50000)\n",
    "#     return training[chosen_ind], target[chosen_ind]\n",
    "\n",
    "# n_targets = target.shape[1]\n",
    "# samples = [get_sample(training, target, i) for i in range(n_targets)]\n",
    "# trainings = [i[0] for i in samples]\n",
    "# targets = [i[1] for i in samples]\n",
    "# big_training = np.concatenate(trainings)\n",
    "# big_target = np.concatenate(targets)\n",
    "# big_training, big_target = util.shuffle_together(big_training, big_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventDataset(Dataset):\n",
    "    def __init__(self, constituents, targets,\n",
    "                 constituents_name = ['j1_pt', 'j1_etarel','j1_phirel']\n",
    "                ):\n",
    "        self.constituents = torch.from_numpy(constituents)\n",
    "        self.targets = torch.from_numpy(targets)\n",
    "        self.constituents_name = constituents_name\n",
    "    def __len__(self):\n",
    "        return self.constituents.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        return self.constituents[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "valid_size = 0.25\n",
    "batch_size = 128\n",
    "pin_memory = False\n",
    "num_workers = 4\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(a,b):\n",
    "    iX = a.shape[1]\n",
    "    iY = a.shape[2]\n",
    "    b_shape = b.shape[1]\n",
    "    a = a.reshape(a.shape[0], iX*iY)\n",
    "    total = np.column_stack((a,b))\n",
    "    np.random.shuffle(total)\n",
    "    a = total[:,:iX*iY]\n",
    "    b = total[:,iX*iY:iX*iY+b_shape]\n",
    "    a = a.reshape(a.shape[0],iX, iY)\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pt_constituents, mass_targets = shuffle(sorted_pt_constituents, mass_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19742\n",
      "19742\n"
     ]
    }
   ],
   "source": [
    "#for commenting\n",
    "sorted_pt_constituentsnp= sorted_pt_constituents\n",
    "\n",
    "num_train = sorted_pt_constituentsnp.shape[0]\n",
    "split = int(np.floor(test_size * num_train))\n",
    "#for commenting\n",
    "sorted_indices = list(range(num_train))\n",
    "split_v=int(np.floor(valid_size * (num_train-split)))\n",
    "print(split)\n",
    "print(split_v)\n",
    "train_idx, test_idx = sorted_indices[split:], sorted_indices[:split]\n",
    "train_idx, valid_idx = train_idx[split_v:], train_idx[:split_v]\n",
    "\n",
    "\n",
    "training_data = sorted_pt_constituentsnp[train_idx, :]\n",
    "validation_data = sorted_pt_constituentsnp[valid_idx, :]\n",
    "testing_data = sorted_pt_constituentsnp[test_idx,:]\n",
    "y_train = mass_targets[train_idx, :]\n",
    "y_valid = mass_targets[valid_idx, :]\n",
    "y_test = mass_targets[test_idx, :]\n",
    "\n",
    "\n",
    "# iSplit_1 = int(0.6*mass_targets.shape[0])\n",
    "# iSplit_2 = int(0.8*mass_targets.shape[0])\n",
    "\n",
    "# training_data= sorted_pt_constituents[:iSplit_1, :]\n",
    "# validation_data = sorted_pt_constituents[iSplit_1:iSplit_2, :]\n",
    "# testing_data = sorted_pt_constituents[iSplit_2:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# y_train = mass_targets[:iSplit_1, :]\n",
    "# y_valid = mass_targets[iSplit_1:iSplit_2, :]\n",
    "# y_test = mass_targets[iSplit_2:, :]\n",
    "\n",
    "y_train_1D =np.argmax(y_train, axis=1)\n",
    "y_valid_1D = np.argmax(y_valid, axis=1)\n",
    "y_test_1D = np.argmax(y_test, axis=1)\n",
    "# print(y_train_1D.shape)\n",
    "# print(y_train_1D[:10])\n",
    "train = EventDataset(training_data,y_train_1D)  \n",
    "valid = EventDataset(validation_data,y_valid_1D)  \n",
    "\n",
    "\n",
    "#get the seq length\n",
    "# t_seq_length= [ arr[i] for i in train_idx]\n",
    "# v_seq_length= [ arr[i] for i in valid_idx]\n",
    "# ts_seq_length=[ arr[i] for i in test_idx]\n",
    "\n",
    "# print(len(t_seq_length[0:1024]))\n",
    "# print(train_idx[:10])\n",
    "# print(arr[train_idx[:10]])\n",
    "\n",
    "print(\"Load dataset\")\n",
    "train_loader = DataLoader(dataset = train, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "valid_loader = DataLoader(dataset = valid, batch_size = batch_size, shuffle = True, num_workers = 4) \n",
    "data_loader = {\"training\" :train_loader, \"validation\" : valid_loader} \n",
    "print(\"Finished\")\n",
    "####### finished "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs, model, criterion, optimizer,scheduler,volatile=False):\n",
    "\n",
    "    best_model = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    train_losses ,val_losses = [],[]\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('_' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['training', 'validation']:\n",
    "            if phase == 'training':\n",
    "                model.train() # Set model to training mode\n",
    "            else:\n",
    "                model.eval() # Set model to evaluate mode\n",
    "                volatile=True\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "            for batch_idx, (x_data, y_data) in enumerate(data_loader[phase]):\n",
    "                x_data, y_data = Variable(x_data.cuda().type(torch.cuda.FloatTensor),volatile),Variable(y_data.cuda().type(torch.cuda.LongTensor))\n",
    "#                 beg = batch_size*batch_idx\n",
    "#                 end = min((batch_idx+1)*batch_size, split)\n",
    "#                 print(\"Beg is %d\" % beg)\n",
    "#                 print(\"Beg is %d\" % end)\n",
    "#                 if phase == 'training':\n",
    "#                     x_data = torch.nn.utils.rnn.pack_padded_sequence(x_data, t_seq_length[beg:end], batch_first=True)\n",
    "#                 else:\n",
    "#                     x_data = torch.nn.utils.rnn.pack_padded_sequence(x_data, v_seq_length[beg:end], batch_first=True)\n",
    "                if phase == 'training':\n",
    "                    optimizer.zero_grad()\n",
    "                # forwardgyg\n",
    "                outputs = model(x_data)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, y_data)\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'training':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds == y_data.data)\n",
    "                #print(\"I finished %d batch\" % batch_idx)\n",
    "            \n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "            epoch_acc = 100. * running_corrects / len(data_loader[phase].dataset)\n",
    "            if phase == 'training':\n",
    "                train_losses.append(epoch_loss)\n",
    "            else:\n",
    "                scheduler.step(epoch_loss)\n",
    "                val_losses.append(epoch_loss)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'validation' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model = model.state_dict()\n",
    "\n",
    "            print()\n",
    "\n",
    "    \n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "    return model,train_losses ,val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "__________\n",
      "training Loss: 0.0110 Acc: 33.9226\n",
      "\n",
      "validation Loss: 0.0104 Acc: 39.0690\n",
      "\n",
      "Epoch 1/199\n",
      "__________\n",
      "training Loss: 0.0101 Acc: 40.4333\n",
      "\n",
      "validation Loss: 0.0100 Acc: 41.3585\n",
      "\n",
      "Epoch 2/199\n",
      "__________\n",
      "training Loss: 0.0099 Acc: 41.9579\n",
      "\n",
      "validation Loss: 0.0099 Acc: 42.1538\n",
      "\n",
      "Epoch 3/199\n",
      "__________\n",
      "training Loss: 0.0099 Acc: 42.4189\n",
      "\n",
      "validation Loss: 0.0099 Acc: 41.9461\n",
      "\n",
      "Epoch 4/199\n",
      "__________\n",
      "training Loss: 0.0098 Acc: 42.5067\n",
      "\n",
      "validation Loss: 0.0099 Acc: 42.5438\n",
      "\n",
      "Epoch 5/199\n",
      "__________\n",
      "training Loss: 0.0098 Acc: 43.0622\n",
      "\n",
      "validation Loss: 0.0099 Acc: 43.0909\n",
      "\n",
      "Epoch 6/199\n",
      "__________\n",
      "training Loss: 0.0097 Acc: 44.1056\n",
      "\n",
      "validation Loss: 0.0097 Acc: 44.4838\n",
      "\n",
      "Epoch 7/199\n",
      "__________\n",
      "training Loss: 0.0095 Acc: 46.0018\n",
      "\n",
      "validation Loss: 0.0094 Acc: 47.2039\n",
      "\n",
      "Epoch 8/199\n",
      "__________\n",
      "training Loss: 0.0093 Acc: 48.0178\n",
      "\n",
      "validation Loss: 0.0094 Acc: 48.0448\n",
      "\n",
      "Epoch 9/199\n",
      "__________\n",
      "training Loss: 0.0092 Acc: 48.9903\n",
      "\n",
      "validation Loss: 0.0091 Acc: 48.8907\n",
      "\n",
      "Epoch 10/199\n",
      "__________\n",
      "training Loss: 0.0091 Acc: 49.6640\n",
      "\n",
      "validation Loss: 0.0090 Acc: 49.7214\n",
      "\n",
      "Epoch 11/199\n",
      "__________\n",
      "training Loss: 0.0090 Acc: 49.9510\n",
      "\n",
      "validation Loss: 0.0091 Acc: 49.5745\n",
      "\n",
      "Epoch 12/199\n",
      "__________\n",
      "training Loss: 0.0089 Acc: 50.3698\n",
      "\n",
      "validation Loss: 0.0092 Acc: 49.3060\n",
      "\n",
      "Epoch 13/199\n",
      "__________\n",
      "training Loss: 0.0089 Acc: 50.7024\n",
      "\n",
      "validation Loss: 0.0089 Acc: 50.2685\n",
      "\n",
      "Epoch 14/199\n",
      "__________\n",
      "training Loss: 0.0089 Acc: 50.8392\n",
      "\n",
      "validation Loss: 0.0094 Acc: 46.7480\n",
      "\n",
      "Epoch 15/199\n",
      "__________\n",
      "training Loss: 0.0089 Acc: 51.1211\n",
      "\n",
      "validation Loss: 0.0088 Acc: 51.0131\n",
      "\n",
      "Epoch 16/199\n",
      "__________\n",
      "training Loss: 0.0088 Acc: 51.2309\n",
      "\n",
      "validation Loss: 0.0088 Acc: 51.2765\n",
      "\n",
      "Epoch 17/199\n",
      "__________\n",
      "training Loss: 0.0088 Acc: 51.2258\n",
      "\n",
      "validation Loss: 0.0088 Acc: 51.0333\n",
      "\n",
      "Epoch 18/199\n",
      "__________\n",
      "training Loss: 0.0088 Acc: 51.3119\n",
      "\n",
      "validation Loss: 0.0087 Acc: 51.6057\n",
      "\n",
      "Epoch 19/199\n",
      "__________\n",
      "training Loss: 0.0088 Acc: 51.5939\n",
      "\n",
      "validation Loss: 0.0088 Acc: 51.2410\n",
      "\n",
      "Epoch 20/199\n",
      "__________\n",
      "training Loss: 0.0087 Acc: 51.6378\n",
      "\n",
      "validation Loss: 0.0087 Acc: 51.5348\n",
      "\n",
      "Epoch 21/199\n",
      "__________\n",
      "training Loss: 0.0088 Acc: 51.6733\n",
      "\n",
      "validation Loss: 0.0087 Acc: 51.4841\n",
      "\n",
      "Epoch 22/199\n",
      "__________\n",
      "training Loss: 0.0087 Acc: 51.7492\n",
      "\n",
      "validation Loss: 0.0087 Acc: 51.4183\n",
      "\n",
      "Epoch 23/199\n",
      "__________\n",
      "training Loss: 0.0087 Acc: 51.6091\n",
      "\n",
      "validation Loss: 0.0087 Acc: 51.9603\n",
      "\n",
      "Epoch 24/199\n",
      "__________\n"
     ]
    }
   ],
   "source": [
    "model_output,train_losses,val_losses = train_model(epochs,model,criterion,optimizer,scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.plot()\n",
    "plt.yscale('log')\n",
    "plt.title('Interaction Network')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_tens=Variable(torch.from_numpy(testing_data),volatile=True).float().cuda()\n",
    "#packed_cons_test = torch.nn.utils.rnn.pack_padded_sequence(testing_data_tens, ts_seq_length, batch_first=True)\n",
    "labels = ['j_g', 'j_q', 'j_w', 'j_z', 'j_t']\n",
    "labels_val = y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame()\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc1 = {}\n",
    "predict_test = model_output(testing_data_tens)\n",
    "predict_test= predict_test.data.cpu().numpy()\n",
    "plt.figure()\n",
    "for i, label in enumerate(labels):\n",
    "        df[label] = labels_val[:,i]\n",
    "        df[label + '_pred'] = predict_test[:,i]\n",
    "\n",
    "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
    "\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        plt.plot(tpr[label],fpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
    "df.to_csv(\"GRU_without_pack_3.csv\", sep='\\t')\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.0001,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper left')\n",
    "#plt.savefig('%s/ROC.pdf'%(options.outputDir))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# N=3\n",
    "# receiver_sender_list = [i for i in itertools.product(range(N), range(N)) if i[0]!=i[1]]\n",
    "# print(receiver_sender_list)\n",
    "# Nr= N*(N-1)\n",
    "# Rr = torch.zeros(N, Nr)\n",
    "# Rs = torch.zeros(N, Nr)\n",
    "# receiver_sender_list = [i for i in itertools.product(range(N), range(N)) if i[0]!=i[1]]\n",
    "# for i, (r, s) in enumerate(receiver_sender_list):\n",
    "#     Rr[r, i] = 1\n",
    "#     Rs[s, i] = 1\n",
    "# print(Rr)\n",
    "# print(Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_shape = Rr.size()\n",
    "# y_shape = Rs.size()\n",
    "# x=Rr\n",
    "# y=Rs\n",
    "# print(x_shape)\n",
    "# print(y_shape)\n",
    "# torch.mm(x.view(-1, x_shape[2]), y).view(-1, x_shape[1], y_shape[1])      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
